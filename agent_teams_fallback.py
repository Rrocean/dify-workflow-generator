"""
Agent Teams Fallback - æ—  Opus 4.6 æ—¶çš„æ›¿ä»£æ–¹æ¡ˆ

æ–¹æ¡ˆ:
1. ä½¿ç”¨ Claude 3.5 Sonnet + å¹¶è¡Œåè°ƒ
2. ä½¿ç”¨æœ¬åœ°å¤šè¿›ç¨‹æ¨¡æ‹Ÿ Agent Teams
3. ä½¿ç”¨ OpenAI/å…¶ä»– LLM å®ç°å¤šä»£ç†
4. ä½¿ç”¨ Sequential æ¨¡å¼ (å•ä»£ç†è¿­ä»£)
"""

import asyncio
import json
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from enum import Enum


class FallbackMode(Enum):
    """é™çº§æ¨¡å¼"""
    SEQUENTIAL = "sequential"           # å•ä»£ç†é¡ºåºæ‰§è¡Œ
    PARALLEL_SONNET = "parallel_sonnet" # Claude 3.5 Sonnet å¹¶è¡Œ
    MULTI_LLM = "multi_llm"             # å¤š LLM æ··åˆ (Claude + GPT)
    LOCAL_MOCK = "local_mock"           # æœ¬åœ°æ¨¡æ‹Ÿæ¨¡å¼


@dataclass
class SimpleAgent:
    """ç®€åŒ–ç‰ˆä»£ç†"""
    id: str
    name: str
    role: str
    model: str  # claude-3-5-sonnet, gpt-4, etc.
    system_prompt: str


class AgentTeamsFallback:
    """
    Agent Teams é™çº§å®ç°
    æ— éœ€ Opus 4.6ï¼Œä½¿ç”¨æ ‡å‡† API å®ç°ç±»ä¼¼åŠŸèƒ½
    """

    def __init__(
        self,
        anthropic_key: Optional[str] = None,
        openai_key: Optional[str] = None,
        mode: FallbackMode = FallbackMode.PARALLEL_SONNET
    ):
        self.anthropic_key = anthropic_key
        self.openai_key = openai_key
        self.mode = mode
        self.agents: List[SimpleAgent] = []

    # ==================== æ–¹æ¡ˆ 1: é¡ºåºæ‰§è¡Œ (æ— éœ€å¹¶è¡Œ) ====================

    async def sequential_development(self, requirements: str) -> Dict[str, Any]:
        """
        æ–¹æ¡ˆ 1: å•ä»£ç†é¡ºåºæ‰§è¡Œ

        ä¼˜ç‚¹:
        - åªéœ€è¦ä¸€ä¸ª LLM API Key
        - ç®€å•å¯é 
        - æˆæœ¬ä½

        ç¼ºç‚¹:
        - æ¯”å¹¶è¡Œæ…¢
        - æ— çœŸæ­£å¹¶è¡Œä¼˜åŠ¿
        """
        print("\n[æ–¹æ¡ˆ 1] é¡ºåºæ‰§è¡Œæ¨¡å¼ (Sequential)")
        print("=" * 60)

        results = {}

        # ä½¿ç”¨å•ä¸ªä»£ç†ï¼Œä½†åˆ†é˜¶æ®µæ‰§è¡Œ
        stages = [
            ("design", "ä½œä¸ºæ¶æ„å¸ˆï¼Œè®¾è®¡å·¥ä½œæµç»“æ„"),
            ("implementation", "ä½œä¸ºå¼€å‘è€…ï¼Œå®ç°èŠ‚ç‚¹ä»£ç "),
            ("validation", "ä½œä¸ºéªŒè¯è€…ï¼Œæ£€æŸ¥ä»£ç æ­£ç¡®æ€§"),
            ("optimization", "ä½œä¸ºä¼˜åŒ–å¸ˆï¼Œæå‡æ€§èƒ½"),
            ("documentation", "ä½œä¸ºæ–‡æ¡£ç¼–å†™è€…ï¼Œç”Ÿæˆæ–‡æ¡£"),
        ]

        context = f"åŸå§‹éœ€æ±‚: {requirements}\n\n"

        for stage_name, stage_prompt in stages:
            print(f"\nâ–¶ï¸  é˜¶æ®µ: {stage_name}")

            # æ„å»ºæç¤ºè¯
            prompt = f"""{stage_prompt}

{context}

è¯·å®Œæˆæ­¤é˜¶æ®µä»»åŠ¡ï¼Œè¾“å‡ºç»“æ„åŒ–ç»“æœã€‚"""

            # è°ƒç”¨ LLM (Claude 3.5 Sonnet æˆ– GPT-4)
            result = await self._call_llm(prompt, model="claude-3-5-sonnet-20241022")

            results[stage_name] = result
            context += f"\n\n[{stage_name}]\n{result}\n"

            print(f"âœ… å®Œæˆ: {stage_name}")

        return results

    # ==================== æ–¹æ¡ˆ 2: å¹¶è¡Œæ‰§è¡Œ (Claude 3.5 Sonnet) ====================

    async def parallel_with_sonnet(self, requirements: str) -> Dict[str, Any]:
        """
        æ–¹æ¡ˆ 2: ä½¿ç”¨ Claude 3.5 Sonnet å¹¶è¡Œæ‰§è¡Œ

        ä¼˜ç‚¹:
        - çœŸæ­£çš„å¹¶è¡Œæ‰§è¡Œ
        - é€Ÿåº¦æå‡ 3-5 å€
        - æ— éœ€ Opus 4.6

        ç¼ºç‚¹:
        - API è°ƒç”¨æ¬¡æ•°å¤š
        - æˆæœ¬ç¨é«˜
        """
        print("\n[æ–¹æ¡ˆ 2] å¹¶è¡Œæ‰§è¡Œæ¨¡å¼ (Parallel with Sonnet)")
        print("=" * 60)

        # å®šä¹‰ç‹¬ç«‹å¯å¹¶è¡Œçš„ä»»åŠ¡
        parallel_tasks = {
            "architecture": f"è®¾è®¡å·¥ä½œæµæ¶æ„:\n{requirements}",
            "node_design": f"è®¾è®¡èŠ‚ç‚¹ç»“æ„:\n{requirements}",
            "api_design": f"è®¾è®¡ API æ¥å£:\n{requirements}",
        }

        # å¹¶è¡Œæ‰§è¡Œç¬¬ä¸€æ³¢ (æ— ä¾èµ–)
        print("\nğŸš€ ç¬¬ä¸€æ³¢å¹¶è¡Œä»»åŠ¡ (æ¶æ„è®¾è®¡)")
        first_wave = await asyncio.gather(*[
            self._call_llm(prompt, model="claude-3-5-sonnet-20241022")
            for prompt in parallel_tasks.values()
        ])

        wave1_results = dict(zip(parallel_tasks.keys(), first_wave))

        # æ„å»ºä¸Šä¸‹æ–‡
        context = f"""åŸå§‹éœ€æ±‚:
{requirements}

æ¶æ„è®¾è®¡:
{wave1_results['architecture']}

èŠ‚ç‚¹è®¾è®¡:
{wave1_results['node_design']}

API è®¾è®¡:
{wave1_results['api_design']}"""

        # å¹¶è¡Œæ‰§è¡Œç¬¬äºŒæ³¢ (ä¾èµ–ç¬¬ä¸€æ³¢)
        print("\nğŸš€ ç¬¬äºŒæ³¢å¹¶è¡Œä»»åŠ¡ (å®ç°)")
        second_wave_tasks = {
            "backend": f"æ ¹æ®ä»¥ä¸‹è®¾è®¡å®ç°åç«¯ä»£ç :\n{context}",
            "frontend": f"æ ¹æ®ä»¥ä¸‹è®¾è®¡å®ç°å‰ç«¯ä»£ç :\n{context}",
            "tests": f"æ ¹æ®ä»¥ä¸‹è®¾è®¡ç¼–å†™æµ‹è¯•:\n{context}",
        }

        second_wave = await asyncio.gather(*[
            self._call_llm(prompt, model="claude-3-5-sonnet-20241022")
            for prompt in second_wave_tasks.values()
        ])

        wave2_results = dict(zip(second_wave_tasks.keys(), second_wave))

        # æœ€ç»ˆæ•´åˆ
        print("\nğŸš€ æœ€ç»ˆæ•´åˆ")
        final_prompt = f"""æ•´åˆä»¥ä¸‹å®ç°ä¸ºå®Œæ•´å·¥ä½œæµ:

åç«¯:
{wave2_results['backend']}

å‰ç«¯:
{wave2_results['frontend']}

æµ‹è¯•:
{wave2_results['tests']}

è¯·è¾“å‡ºæœ€ç»ˆå®Œæ•´çš„å·¥ä½œæµå®šä¹‰ã€‚"""

        final_result = await self._call_llm(final_prompt, model="claude-3-5-sonnet-20241022")

        return {
            **wave1_results,
            **wave2_results,
            "final_workflow": final_result
        }

    # ==================== æ–¹æ¡ˆ 3: å¤š LLM æ··åˆ ====================

    async def multi_llm_coordination(self, requirements: str) -> Dict[str, Any]:
        """
        æ–¹æ¡ˆ 3: ä½¿ç”¨å¤šä¸ª LLM ååŒ

        Claude 3.5 Sonnet + GPT-4 æ··åˆä½¿ç”¨
        å„å–æ‰€é•¿
        """
        print("\n[æ–¹æ¡ˆ 3] å¤š LLM åè°ƒæ¨¡å¼ (Claude + GPT)")
        print("=" * 60)

        # åˆ†é…ä¸åŒä»»åŠ¡ç»™ä¸åŒæ¨¡å‹
        tasks = {
            "claude": {
                "model": "claude-3-5-sonnet-20241022",
                "prompt": f"ä½¿ç”¨ Claude ä¼˜åŠ¿è¿›è¡Œæ¶æ„è®¾è®¡:\n{requirements}",
                "key": self.anthropic_key
            },
            "gpt4": {
                "model": "gpt-4-turbo-preview",
                "prompt": f"ä½¿ç”¨ GPT-4 ä¼˜åŠ¿è¿›è¡Œä»£ç å®ç°:\n{requirements}",
                "key": self.openai_key
            },
        }

        # å¹¶è¡Œè°ƒç”¨ä¸åŒ API
        async def run_task(name: str, config: dict):
            print(f"\nğŸ¤– {name} ({config['model']}) å¼€å§‹å·¥ä½œ...")
            result = await self._call_llm_api(
                config['prompt'],
                config['model'],
                config['key']
            )
            print(f"âœ… {name} å®Œæˆ")
            return name, result

        results = await asyncio.gather(*[
            run_task(name, config)
            for name, config in tasks.items()
        ])

        return dict(results)

    # ==================== æ–¹æ¡ˆ 4: æœ¬åœ°æ¨¡æ‹Ÿæ¨¡å¼ ====================

    async def local_mock_mode(self, requirements: str) -> Dict[str, Any]:
        """
        æ–¹æ¡ˆ 4: æœ¬åœ°æ¨¡æ‹Ÿ (æ— éœ€ API)

        ä½¿ç”¨é¢„è®¾æ¨¡æ¿å’Œè§„åˆ™æ¨¡æ‹Ÿ Agent Teams
        é€‚åˆæ¼”ç¤ºå’Œæµ‹è¯•
        """
        print("\n[æ–¹æ¡ˆ 4] æœ¬åœ°æ¨¡æ‹Ÿæ¨¡å¼ (æ— éœ€ API)")
        print("=" * 60)

        # è§£æéœ€æ±‚å…³é”®è¯
        keywords = self._extract_keywords(requirements)

        # æ ¹æ®å…³é”®è¯é€‰æ‹©æ¨¡æ¿
        templates = {
            "translation": self._get_translation_template(),
            "chatbot": self._get_chatbot_template(),
            "summarization": self._get_summarization_template(),
        }

        # æ‰¾åˆ°æœ€åŒ¹é…çš„æ¨¡æ¿
        matched_template = None
        for keyword in keywords:
            if keyword in templates:
                matched_template = templates[keyword]
                break

        if not matched_template:
            matched_template = self._get_generic_template()

        # æ¨¡æ‹Ÿå„ä»£ç†å·¥ä½œ
        results = {
            "design": f"[æ¨¡æ‹Ÿ] æ¶æ„å¸ˆè®¾è®¡å·¥ä½œæµ\nåŸºäºéœ€æ±‚: {requirements[:100]}...",
            "implementation": f"[æ¨¡æ‹Ÿ] å¼€å‘è€…å®ç°èŠ‚ç‚¹\nä½¿ç”¨æ¨¡æ¿: {matched_template['name']}",
            "validation": "[æ¨¡æ‹Ÿ] éªŒè¯è€…æ£€æŸ¥ DSL\næ‰€æœ‰æ£€æŸ¥é€šè¿‡ âœ“",
            "optimization": "[æ¨¡æ‹Ÿ] ä¼˜åŒ–å¸ˆå»ºè®®:\n- ä½¿ç”¨ç¼“å­˜å‡å°‘ API è°ƒç”¨\n- æ·»åŠ é‡è¯•æœºåˆ¶",
            "documentation": f"[æ¨¡æ‹Ÿ] ç”Ÿæˆæ–‡æ¡£\nåŸºäºæ¨¡æ¿: {matched_template['description']}",
            "final_workflow": matched_template['workflow']
        }

        # æ¨¡æ‹Ÿæ‰§è¡Œæ—¶é—´
        for key in results:
            await asyncio.sleep(0.2)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
            print(f"âœ… {key} å®Œæˆ")

        return results

    # ==================== è¾…åŠ©æ–¹æ³• ====================

    async def _call_llm(self, prompt: str, model: str = "claude-3-5-sonnet-20241022") -> str:
        """è°ƒç”¨ LLM API"""
        if "claude" in model.lower():
            return await self._call_claude(prompt, model)
        else:
            return await self._call_openai(prompt, model)

    async def _call_claude(self, prompt: str, model: str) -> str:
        """è°ƒç”¨ Claude API"""
        try:
            import anthropic
            client = anthropic.Anthropic(api_key=self.anthropic_key)

            response = client.messages.create(
                model=model,
                max_tokens=4096,
                messages=[{"role": "user", "content": prompt}]
            )

            return response.content[0].text
        except Exception as e:
            return f"[Claude API Error: {e}]"

    async def _call_openai(self, prompt: str, model: str) -> str:
        """è°ƒç”¨ OpenAI API"""
        try:
            import openai
            client = openai.AsyncOpenAI(api_key=self.openai_key)

            response = await client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=4096
            )

            return response.choices[0].message.content
        except Exception as e:
            return f"[OpenAI API Error: {e}]"

    async def _call_llm_api(self, prompt: str, model: str, api_key: str) -> str:
        """é€šç”¨ LLM API è°ƒç”¨"""
        if "claude" in model.lower():
            return await self._call_claude(prompt, model)
        else:
            return await self._call_openai(prompt, model)

    def _extract_keywords(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬æå–å…³é”®è¯"""
        keywords = ["translation", "chatbot", "summarization", "code", "review"]
        found = []
        text_lower = text.lower()
        for kw in keywords:
            if kw in text_lower:
                found.append(kw)
        return found

    def _get_translation_template(self) -> Dict:
        """ç¿»è¯‘å·¥ä½œæµæ¨¡æ¿"""
        return {
            "name": "Translation Workflow",
            "description": "A workflow for translating text between languages",
            "workflow": {
                "version": "0.5.0",
                "nodes": [
                    {"id": "start", "type": "start", "variables": ["text", "target_lang"]},
                    {"id": "llm", "type": "llm", "prompt": "Translate to {{#start.target_lang#}}: {{#start.text#}}"},
                    {"id": "end", "type": "end"}
                ]
            }
        }

    def _get_chatbot_template(self) -> Dict:
        """èŠå¤©æœºå™¨äººæ¨¡æ¿"""
        return {
            "name": "Chatbot Workflow",
            "description": "An AI chatbot with memory support",
            "workflow": {
                "version": "0.5.0",
                "nodes": [
                    {"id": "start", "type": "start", "variables": ["query"]},
                    {"id": "llm", "type": "llm", "prompt": "User: {{#start.query#}}"},
                    {"id": "answer", "type": "answer"}
                ]
            }
        }

    def _get_summarization_template(self) -> Dict:
        """æ‘˜è¦æ¨¡æ¿"""
        return {
            "name": "Summarization Workflow",
            "description": "Summarize long text into key points",
            "workflow": {
                "version": "0.5.0",
                "nodes": [
                    {"id": "start", "type": "start", "variables": ["text"]},
                    {"id": "llm", "type": "llm", "prompt": "Summarize: {{#start.text#}}"},
                    {"id": "end", "type": "end"}
                ]
            }
        }

    def _get_generic_template(self) -> Dict:
        """é€šç”¨æ¨¡æ¿"""
        return {
            "name": "Generic Workflow",
            "description": "A general-purpose workflow",
            "workflow": {
                "version": "0.5.0",
                "nodes": [
                    {"id": "start", "type": "start"},
                    {"id": "llm", "type": "llm"},
                    {"id": "end", "type": "end"}
                ]
            }
        }

    # ==================== ä¸»å…¥å£ ====================

    async def develop(
        self,
        requirements: str,
        mode: Optional[FallbackMode] = None
    ) -> Dict[str, Any]:
        """
        ä¸»å…¥å£ - è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ–¹æ¡ˆ
        """
        mode = mode or self.mode

        print(f"\n{'='*70}")
        print(f"ğŸš€ Agent Teams Fallback - {mode.value}")
        print(f"{'='*70}")

        if mode == FallbackMode.SEQUENTIAL:
            return await self.sequential_development(requirements)

        elif mode == FallbackMode.PARALLEL_SONNET:
            if not self.anthropic_key:
                print("âš ï¸  æ—  Anthropic API Keyï¼Œåˆ‡æ¢åˆ°é¡ºåºæ¨¡å¼")
                return await self.sequential_development(requirements)
            return await self.parallel_with_sonnet(requirements)

        elif mode == FallbackMode.MULTI_LLM:
            if not (self.anthropic_key and self.openai_key):
                print("âš ï¸  éœ€è¦ Anthropic å’Œ OpenAI Keyï¼Œåˆ‡æ¢åˆ° Sonnet å¹¶è¡Œ")
                return await self.parallel_with_sonnet(requirements)
            return await self.multi_llm_coordination(requirements)

        elif mode == FallbackMode.LOCAL_MOCK:
            return await self.local_mock_mode(requirements)

        else:
            raise ValueError(f"Unknown mode: {mode}")


# ==================== ä½¿ç”¨ç¤ºä¾‹ ====================

async def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºå„ç§ fallback æ–¹æ¡ˆ"""

    requirements = """
    åˆ›å»ºä¸€ä¸ªæ™ºèƒ½å®¢æœå·¥ä½œæµï¼Œéœ€è¦ï¼š
    1. æ¥æ”¶å®¢æˆ·æ¶ˆæ¯
    2. åˆ†ææ„å›¾ï¼ˆè®¢å•æŸ¥è¯¢/é€€æ¬¾/äº§å“å’¨è¯¢ï¼‰
    3. æ ¹æ®æ„å›¾è·¯ç”±åˆ°ä¸åŒå¤„ç†èŠ‚ç‚¹
    4. æŸ¥è¯¢æ•°æ®åº“æˆ–çŸ¥è¯†åº“
    5. ç”Ÿæˆå›å¤
    """

    # æ–¹æ¡ˆ 1: é¡ºåºæ¨¡å¼ (åªéœ€ä¸€ä¸ª API Keyï¼Œæœ€çœé’±)
    print("\n" + "="*70)
    print("æ–¹æ¡ˆ 1: é¡ºåºæ¨¡å¼ (æ¨èåˆå­¦è€…)")
    print("="*70)

    fallback1 = AgentTeamsFallback(
        anthropic_key="your-anthropic-key",  # æ›¿æ¢ä¸ºä½ çš„ key
        mode=FallbackMode.SEQUENTIAL
    )
    result1 = await fallback1.develop(requirements)
    print("\nâœ… é¡ºåºæ¨¡å¼å®Œæˆ")

    # æ–¹æ¡ˆ 2: Sonnet å¹¶è¡Œ (éœ€è¦ Anthropic Keyï¼Œé€Ÿåº¦æœ€å¿«)
    print("\n" + "="*70)
    print("æ–¹æ¡ˆ 2: Sonnet å¹¶è¡Œæ¨¡å¼ (æ¨èæœ‰ API Key)")
    print("="*70)

    fallback2 = AgentTeamsFallback(
        anthropic_key="your-anthropic-key",
        mode=FallbackMode.PARALLEL_SONNET
    )
    result2 = await fallback2.develop(requirements)
    print("\nâœ… å¹¶è¡Œæ¨¡å¼å®Œæˆ")

    # æ–¹æ¡ˆ 3: æœ¬åœ°æ¨¡æ‹Ÿ (æ— éœ€ APIï¼Œé€‚åˆæµ‹è¯•)
    print("\n" + "="*70)
    print("æ–¹æ¡ˆ 3: æœ¬åœ°æ¨¡æ‹Ÿæ¨¡å¼ (æ— éœ€ API)")
    print("="*70)

    fallback3 = AgentTeamsFallback(mode=FallbackMode.LOCAL_MOCK)
    result3 = await fallback3.develop(requirements)
    print("\nâœ… æ¨¡æ‹Ÿæ¨¡å¼å®Œæˆ")

    # ä¿å­˜ç»“æœ
    with open("workflow_results.json", "w") as f:
        json.dump({
            "sequential": result1,
            "parallel": result2,
            "mock": result3
        }, f, indent=2, ensure_ascii=False)

    print("\n" + "="*70)
    print("ğŸ“ ç»“æœå·²ä¿å­˜åˆ° workflow_results.json")
    print("="*70)


if __name__ == "__main__":
    print("ğŸš€ Agent Teams Fallback - æ— éœ€ Opus 4.6 çš„å¤šä»£ç†æ–¹æ¡ˆ")
    print("="*70)
    print("\nå¯ç”¨æ–¹æ¡ˆ:")
    print("1. é¡ºåºæ¨¡å¼ (Sequential) - åªéœ€ä¸€ä¸ª API Key")
    print("2. å¹¶è¡Œæ¨¡å¼ (Parallel) - ä½¿ç”¨ Claude 3.5 Sonnet")
    print("3. å¤š LLM æ¨¡å¼ (Multi-LLM) - Claude + GPT æ··åˆ")
    print("4. æœ¬åœ°æ¨¡æ‹Ÿ (Mock) - æ— éœ€ APIï¼Œçº¯æœ¬åœ°")

    asyncio.run(main())
